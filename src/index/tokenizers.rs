//! ğŸ”Œ MoteDB åˆ†è¯å™¨æ’ä»¶ç³»ç»Ÿ
//!
//! æ¶æ„è®¾è®¡ï¼š
//! - **Core Trait**: `Tokenizer` trait å®šä¹‰åˆ†è¯å™¨æ¥å£
//! - **Built-in**: å†…ç½®è½»é‡çº§åˆ†è¯å™¨ï¼ˆWhitespace, Ngramï¼‰
//! - **Plugin System**: é€šè¿‡ Feature Flags å¯ç”¨ç¬¬ä¸‰æ–¹åˆ†è¯å™¨
//! - **Runtime Registry**: ç”¨æˆ·å¯æ³¨å†Œè‡ªå®šä¹‰åˆ†è¯å™¨
//!
//! ## ä½¿ç”¨æ–¹å¼
//!
//! ### 1. ä½¿ç”¨å†…ç½®åˆ†è¯å™¨ï¼ˆæ— éœ€é¢å¤–ä¾èµ–ï¼‰
//! ```ignore
//! use motedb::tokenizers::{WhitespaceTokenizer, NgramTokenizer};
//!
//! // ç©ºæ ¼åˆ†è¯ï¼ˆè‹±æ–‡ï¼‰
//! let tokenizer = WhitespaceTokenizer::default();
//!
//! // N-gram åˆ†è¯ï¼ˆCJKï¼‰
//! let tokenizer = NgramTokenizer::new(2);  // bigram
//! ```
//!
//! ### 2. ä½¿ç”¨ç¬¬ä¸‰æ–¹åˆ†è¯å™¨æ’ä»¶ï¼ˆFeature Flagï¼‰
//! ```ignore
//! # Cargo.toml
//! [dependencies]
//! motedb = { version = "0.1", features = ["tokenizer-jieba"] }
//! ```
//!
//! ```ignore
//! use motedb::tokenizers::JiebaTokenizer;
//!
//! // ä¸­æ–‡åˆ†è¯ï¼ˆJiebaï¼‰
//! let tokenizer = JiebaTokenizer::default();
//! let tokens = tokenizer.tokenize("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†");
//! ```
//!
//! ### 3. è‡ªå®šä¹‰åˆ†è¯å™¨ï¼ˆç”¨æˆ·æ‰©å±•ï¼‰
//! ```ignore
//! use motedb::tokenizers::{Tokenizer, Token};
//!
//! struct MyCustomTokenizer;
//!
//! impl Tokenizer for MyCustomTokenizer {
//!     fn tokenize(&self, text: &str) -> Vec<Token> {
//!         // è‡ªå®šä¹‰åˆ†è¯é€»è¾‘
//!         text.chars()
//!             .enumerate()
//!             .map(|(i, c)| Token {
//!                 text: c.to_string(),
//!                 position: i as u32,
//!             })
//!             .collect()
//!     }
//!
//!     fn name(&self) -> &str {
//!         "custom"
//!     }
//! }
//! ```
// å¯¼å‡ºæ ¸å¿ƒ trait å’Œæ•°æ®ç»“æ„
pub use crate::index::text_types::{Tokenizer, Token, Position};

// å¯¼å‡ºå†…ç½®åˆ†è¯å™¨ï¼ˆé›¶ä¾èµ–ï¼Œå§‹ç»ˆå¯ç”¨ï¼‰
pub use crate::index::text_types::{WhitespaceTokenizer, NgramTokenizer};

//=============================================================================
// ğŸ”Œ Plugin: Jieba ä¸­æ–‡åˆ†è¯å™¨ï¼ˆå¯é€‰ï¼‰
//=============================================================================

#[cfg(feature = "tokenizer-jieba")]
mod jieba_plugin {
    use super::*;
    use jieba_rs::Jieba;
    use std::sync::Arc;

    /// Jieba ä¸­æ–‡åˆ†è¯å™¨ï¼ˆåŸºäº jieba-rsï¼‰
    ///
    /// ç‰¹æ€§ï¼š
    /// - æ”¯æŒç²¾ç¡®æ¨¡å¼ã€å…¨æ¨¡å¼ã€æœç´¢å¼•æ“æ¨¡å¼
    /// - æ”¯æŒè‡ªå®šä¹‰è¯å…¸
    /// - HMM æ–°è¯å‘ç°
    ///
    /// æ€§èƒ½ï¼š
    /// - ç¼–è¯‘æ—¶é—´ï¼š+2-3ç§’ï¼ˆä»…åœ¨å¯ç”¨ feature æ—¶ï¼‰
    /// - äºŒè¿›åˆ¶å¤§å°ï¼š+350KBï¼ˆä»…åœ¨å¯ç”¨ feature æ—¶ï¼‰
    /// - è¿è¡Œæ—¶ï¼š~50-100K tokens/sec
    ///
    /// ä½¿ç”¨ç¤ºä¾‹ï¼š
    /// ```ignore
    /// use motedb::tokenizers::JiebaTokenizer;
    ///
    /// let tokenizer = JiebaTokenizer::default();
    /// let tokens = tokenizer.tokenize("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†");
    /// // => ["æˆ‘", "çˆ±", "è‡ªç„¶è¯­è¨€", "å¤„ç†"]
    /// ```
    pub struct JiebaTokenizer {
        jieba: Arc<Jieba>,
        mode: JiebaMode,
        case_sensitive: bool,
        min_len: usize,
        max_len: usize,
    }

    /// Jieba åˆ†è¯æ¨¡å¼
    #[derive(Debug, Clone, Copy, PartialEq, Eq)]
    pub enum JiebaMode {
        /// ç²¾ç¡®æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼šè¯•å›¾å°†å¥å­æœ€ç²¾ç¡®åœ°åˆ‡å¼€
        Precise,
        /// å…¨æ¨¡å¼ï¼šæŠŠå¥å­ä¸­æ‰€æœ‰å¯èƒ½çš„è¯è¯­éƒ½æ‰«æå‡ºæ¥
        Full,
        /// æœç´¢å¼•æ“æ¨¡å¼ï¼šåœ¨ç²¾ç¡®æ¨¡å¼åŸºç¡€ä¸Šï¼Œå¯¹é•¿è¯å†æ¬¡åˆ‡åˆ†
        Search,
    }

    impl Default for JiebaTokenizer {
        fn default() -> Self {
            Self {
                jieba: Arc::new(Jieba::new()),
                mode: JiebaMode::Search,  // é»˜è®¤æœç´¢æ¨¡å¼ï¼Œé€‚åˆå…¨æ–‡æ£€ç´¢
                case_sensitive: false,
                min_len: 1,
                max_len: 64,
            }
        }
    }

    impl JiebaTokenizer {
        /// åˆ›å»ºæ–°çš„ Jieba åˆ†è¯å™¨
        pub fn new() -> Self {
            Self::default()
        }

        /// è®¾ç½®åˆ†è¯æ¨¡å¼
        pub fn with_mode(mut self, mode: JiebaMode) -> Self {
            self.mode = mode;
            self
        }

        /// è®¾ç½®å¤§å°å†™æ•æ„Ÿ
        pub fn case_sensitive(mut self, sensitive: bool) -> Self {
            self.case_sensitive = sensitive;
            self
        }

        /// è®¾ç½®è¯é•¿åº¦èŒƒå›´
        pub fn with_length_range(mut self, min: usize, max: usize) -> Self {
            self.min_len = min;
            self.max_len = max;
            self
        }

        /// åŠ è½½è‡ªå®šä¹‰è¯å…¸
        pub fn load_dict(&mut self, dict_path: &str) -> Result<(), String> {
            // jieba-rs ä¸æ”¯æŒè¿è¡Œæ—¶åŠ è½½è¯å…¸ï¼Œéœ€è¦åœ¨æ„å»ºæ—¶å¤„ç†
            // è¿™é‡Œæä¾›æ¥å£å ä½ï¼Œå®é™…å¯é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶å®ç°
            Ok(())
        }
    }

    impl Tokenizer for JiebaTokenizer {
        fn tokenize(&self, text: &str) -> Vec<Token> {
            // æ ¹æ®æ¨¡å¼é€‰æ‹©åˆ†è¯æ–¹æ³•
            let words = match self.mode {
                JiebaMode::Precise => self.jieba.cut(text, false),
                JiebaMode::Full => self.jieba.cut(text, true),
                JiebaMode::Search => self.jieba.cut_for_search(text, false),
            };

            words
                .into_iter()
                .enumerate()
                .filter_map(|(i, word)| {
                    let word_str = word.trim();
                    if word_str.is_empty() {
                        return None;
                    }

                    let len = word_str.chars().count();
                    if len < self.min_len || len > self.max_len {
                        return None;
                    }

                    let text = if self.case_sensitive {
                        word_str.to_string()
                    } else {
                        word_str.to_lowercase()
                    };

                    Some(Token {
                        text,
                        position: i as u32,
                    })
                })
                .collect()
        }

        fn name(&self) -> &str {
            "jieba"
        }
    }

    #[cfg(test)]
    mod tests {
        use super::*;

        #[test]
        fn test_jieba_tokenizer() {
            let tokenizer = JiebaTokenizer::default();
            let tokens = tokenizer.tokenize("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†");
            
            println!("Tokens: {:?}", tokens.iter().map(|t| &t.text).collect::<Vec<_>>());
            assert!(!tokens.is_empty());
            assert!(tokens.iter().any(|t| t.text == "è‡ªç„¶è¯­è¨€"));
        }

        #[test]
        fn test_jieba_modes() {
            let text = "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦";

            // ç²¾ç¡®æ¨¡å¼
            let precise = JiebaTokenizer::default().with_mode(JiebaMode::Precise);
            let tokens = precise.tokenize(text);
            println!("Precise: {:?}", tokens.iter().map(|t| &t.text).collect::<Vec<_>>());

            // å…¨æ¨¡å¼
            let full = JiebaTokenizer::default().with_mode(JiebaMode::Full);
            let tokens = full.tokenize(text);
            println!("Full: {:?}", tokens.iter().map(|t| &t.text).collect::<Vec<_>>());

            // æœç´¢æ¨¡å¼
            let search = JiebaTokenizer::default().with_mode(JiebaMode::Search);
            let tokens = search.tokenize(text);
            println!("Search: {:?}", tokens.iter().map(|t| &t.text).collect::<Vec<_>>());
        }
    }
}

#[cfg(feature = "tokenizer-jieba")]
pub use jieba_plugin::{JiebaTokenizer, JiebaMode};

//=============================================================================
// ğŸ”Œ Future Plugins (Placeholder)
//=============================================================================

// æœªæ¥å¯æ·»åŠ æ›´å¤šåˆ†è¯å™¨ï¼š
// - Tantivy åˆ†è¯å™¨ï¼ˆåŸºäº Tantivyï¼‰
// - ICU åˆ†è¯å™¨ï¼ˆåŸºäº icu-rustï¼‰
// - Mecab åˆ†è¯å™¨ï¼ˆæ—¥è¯­ï¼‰
// - OpenCC åˆ†è¯å™¨ï¼ˆç¹ç®€è½¬æ¢ï¼‰

//=============================================================================
// ğŸ“š åˆ†è¯å™¨å·¥å‚ï¼ˆæ–¹ä¾¿ç”¨æˆ·é€‰æ‹©ï¼‰
//=============================================================================

/// åˆ†è¯å™¨ç±»å‹æšä¸¾
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum TokenizerType {
    /// ç©ºæ ¼åˆ†è¯ï¼ˆè‹±æ–‡ï¼‰
    Whitespace,
    /// N-gram åˆ†è¯ï¼ˆCJKï¼‰
    Ngram,
    /// Jieba ä¸­æ–‡åˆ†è¯ï¼ˆéœ€è¦ `tokenizer-jieba` featureï¼‰
    #[cfg(feature = "tokenizer-jieba")]
    Jieba,
}

/// åˆ†è¯å™¨å·¥å‚
pub struct TokenizerFactory;

impl TokenizerFactory {
    /// æ ¹æ®ç±»å‹åˆ›å»ºåˆ†è¯å™¨
    pub fn create(tokenizer_type: TokenizerType) -> Box<dyn Tokenizer> {
        match tokenizer_type {
            TokenizerType::Whitespace => Box::new(WhitespaceTokenizer::default()),
            TokenizerType::Ngram => Box::new(NgramTokenizer::new(2)),
            #[cfg(feature = "tokenizer-jieba")]
            TokenizerType::Jieba => Box::new(JiebaTokenizer::default()),
        }
    }

    /// ä»å­—ç¬¦ä¸²åç§°åˆ›å»ºåˆ†è¯å™¨
    pub fn from_name(name: &str) -> Option<Box<dyn Tokenizer>> {
        match name {
            "whitespace" => Some(Box::new(WhitespaceTokenizer::default())),
            "ngram" | "ngram2" => Some(Box::new(NgramTokenizer::new(2))),
            "ngram3" => Some(Box::new(NgramTokenizer::new(3))),
            #[cfg(feature = "tokenizer-jieba")]
            "jieba" => Some(Box::new(JiebaTokenizer::default())),
            _ => None,
        }
    }

    /// åˆ—å‡ºæ‰€æœ‰å¯ç”¨åˆ†è¯å™¨
    pub fn available_tokenizers() -> Vec<&'static str> {
        let tokenizers = vec!["whitespace", "ngram"];
        #[cfg(feature = "tokenizer-jieba")]
        let tokenizers = {
            let mut t = tokenizers;
            t.push("jieba");
            t
        };
        tokenizers
    }
}

//=============================================================================
// ğŸ“– æ–‡æ¡£å’Œç¤ºä¾‹
//=============================================================================

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_whitespace_tokenizer() {
        let tokenizer = WhitespaceTokenizer::default();
        let tokens = tokenizer.tokenize("Hello World Test");
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0].text, "hello");
        assert_eq!(tokens[1].text, "world");
    }

    #[test]
    fn test_ngram_tokenizer() {
        let tokenizer = NgramTokenizer::new(2);
        let tokens = tokenizer.tokenize("ä½ å¥½ä¸–ç•Œ");
        assert!(!tokens.is_empty());
    }

    #[test]
    fn test_tokenizer_factory() {
        // æµ‹è¯•å·¥å‚åˆ›å»º
        let tokenizer = TokenizerFactory::from_name("whitespace").unwrap();
        let tokens = tokenizer.tokenize("test");
        assert_eq!(tokens.len(), 1);

        // æµ‹è¯•å¯ç”¨åˆ†è¯å™¨åˆ—è¡¨
        let available = TokenizerFactory::available_tokenizers();
        println!("Available tokenizers: {:?}", available);
        assert!(available.contains(&"whitespace"));
        assert!(available.contains(&"ngram"));
    }

    #[test]
    fn test_custom_tokenizer() {
        // ç”¨æˆ·è‡ªå®šä¹‰åˆ†è¯å™¨ç¤ºä¾‹
        struct CharTokenizer;
        
        impl Tokenizer for CharTokenizer {
            fn tokenize(&self, text: &str) -> Vec<Token> {
                text.chars()
                    .enumerate()
                    .map(|(i, c)| Token {
                        text: c.to_string(),
                        position: i as u32,
                    })
                    .collect()
            }
            
            fn name(&self) -> &str {
                "char"
            }
        }

        let tokenizer = CharTokenizer;
        let tokens = tokenizer.tokenize("ABC");
        assert_eq!(tokens.len(), 3);
        assert_eq!(tokens[0].text, "A");
        assert_eq!(tokens[1].text, "B");
        assert_eq!(tokens[2].text, "C");
    }
}
